{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_spd_matrix\n",
    "from scipy.stats import multivariate_normal\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class k_means():\n",
    "    \n",
    "    def __init__(self,n_components,dataset):\n",
    "        \n",
    "        self.n_components = n_components\n",
    "        self.dataset = dataset\n",
    "        self.covariance_matrix = []\n",
    "        self.weights = []\n",
    "        self.cluster_centroid = []\n",
    "        self.scaler = StandardScaler()\n",
    "        self.fit_kmeans()\n",
    "        \n",
    "    def final_covariance_matrix(self,points):\n",
    "        '''For generating the covariance matrix of each cluster'''\n",
    "        for cluster,points in points.items():\n",
    "            self.covariance_matrix.append(np.cov(np.asarray(points).T))\n",
    "        \n",
    "        \n",
    "    def cluster_assigment_step(self):\n",
    "        '''Cluster assignment step based on euclidean distance b/w point and cluster'''\n",
    "        \n",
    "        cluster_assignment = {k:[] for k in range(self.n_components)}\n",
    "        for point in self.dataset:\n",
    "            cluster_dist =[]\n",
    "            for cluster in self.cluster_centroid:\n",
    "                cluster_dist.append(np.sqrt(np.sum((point-cluster)**2)))\n",
    "            located_cluster = cluster_dist.index(min(cluster_dist))\n",
    "            cluster_assignment[located_cluster].append(point)\n",
    "\n",
    "\n",
    "        groupby_points_previous = {cluster:len(points) for cluster,points in cluster_assignment.items()}\n",
    "        \n",
    "        return groupby_points_previous,cluster_assignment\n",
    "\n",
    "    def scaled_dataset(self):\n",
    "        '''Scaling the dataset'''\n",
    "        \n",
    "        self.dataset = self.scaler.fit_transform(self.dataset)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def fit_kmeans(self):\n",
    "        '''Fitting k-means with convergence criterion that no. of points assigned to different cluster do not change'''\n",
    "        #sclaing of the dataset\n",
    "        self.scaled_dataset()\n",
    "        \n",
    "        #initialisation of cluster centroid\n",
    "        \n",
    "        np.random.shuffle(dataset)\n",
    "        \n",
    "        self.cluster_centroid = [self.dataset[i] for i in range(self.n_components)]\n",
    "        \n",
    "        Counter_list = []\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            # Assignment step\n",
    "            \n",
    "            grouped_cluster_previous,assigned_clusters_previous = self.cluster_assigment_step()\n",
    "            \n",
    "            # Cluster centroid update step \n",
    "            self.cluster_centroid = [np.mean(np.asarray(points),axis = 0) for cluster,points in assigned_clusters_previous.items()]\n",
    "            \n",
    "            groupby_points_next,assigned_clusters_new = self.cluster_assigment_step()\n",
    "            \n",
    "            # check for no changes in assignment of points to the cluster\n",
    "            if groupby_points_next==grouped_cluster_previous:\n",
    "                break\n",
    "        #calcuation of weights after clustering using kmeans, for initialising for GMM\n",
    "        self.weights = [value/sum([value for value in groupby_points_next.values()]) for key,value in groupby_points_next.items()]\n",
    "        self.final_covariance_matrix(assigned_clusters_new)\n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gaussian_mixture_models(k_means):\n",
    "    \n",
    "    def __init__(self,n_components,dataset,max_iter=100,conv_threshold=.001):\n",
    "        # Initialising cluster centroids and cluster covariances matrix using kmeans\n",
    "        k_means.__init__(self,n_components,dataset) \n",
    "        self.max_iter = max_iter\n",
    "        self.conv_threshold = conv_threshold\n",
    "        self.dim = self.dataset.shape[1]\n",
    "        self.n_iter_ = 0\n",
    "        self.converged_ = False\n",
    "        self.eps = 0 # for zero division error\n",
    "        self.log_likehood= []\n",
    "    \n",
    "    \n",
    "    def calculate_likelihood(self):\n",
    "        '''calclulating pdf of samples using scipy multivariate normal distribution'''\n",
    "        likelihood = []\n",
    "        responsbility = []\n",
    "        for i in range(self.n_components):\n",
    "            likelihood.append(multivariate_normal.pdf(self.dataset,self.cluster_centroid[i],self.covariance_matrix[i]))\n",
    "        for i in range(self.n_components):\n",
    "            responsbility.append((self.weights[i]*likelihood[i])/(np.sum([self.weights[i]*likelihood[i] for i in range(self.n_components)],axis = 0)+self.eps))\n",
    "        return responsbility\n",
    "    \n",
    "    def m_step(self,responsbility):\n",
    "        '''Parameters update step'''\n",
    "        \n",
    "        for i in range(self.n_components):\n",
    "            self.cluster_centroid[i]=np.sum(responsbility[i].reshape(len(self.dataset),1)*self.dataset,axis = 0)/(np.sum(responsbility[i])+self.eps)\n",
    "            self.covariance_matrix[i] = np.dot((responsbility[i].reshape(len(self.dataset),1)*(self.dataset-self.cluster_centroid[i])).T,\n",
    "                                                    (self.dataset-self.cluster_centroid[i]))/(np.sum(responsbility[i])+self.eps)\n",
    "            self.weights[i]=np.mean(responsbility[i])\n",
    "                                             \n",
    "        \n",
    "    \n",
    "    def calculate_log_likelihood(self,mean_vector,covariance_matrix,weights):\n",
    "        '''Calculation of complete log-likelihood across all the clusters and data points'''\n",
    "        \n",
    "        sum_across_cluster = np.sum([weights[i]*multivariate_normal.pdf(self.dataset,mean_vector[i],covariance_matrix[i]) for i in range(self.n_components)],1)\n",
    "        \n",
    "        \n",
    "        sum_across_data_points = np.sum(np.log(sum_across_cluster))\n",
    "        \n",
    "        return sum_across_data_points\n",
    "    \n",
    "    def fit(self):\n",
    "        '''Iterative training of parameters using Expectation-Maximisation algorithm'''\n",
    "        \n",
    "        for i in range(self.max_iter):\n",
    "            initial_likelihood = self.calculate_log_likelihood(self.cluster_centroid,self.covariance_matrix,self.weights)\n",
    "            self.log_likehood.append(initial_likelihood)    \n",
    "            \n",
    "            self.n_iter_+=1\n",
    "            \n",
    "            # calculate responsbility (E-step)\n",
    "            responsbility = self.calculate_likelihood()\n",
    "            \n",
    "            # calculate the updated parameters (M-step)\n",
    "            self.m_step(responsbility)\n",
    "            \n",
    "            # calculate new loglikelihood for checking conevergence\n",
    "            final_likelihood = self.calculate_log_likelihood(self.cluster_centroid,self.covariance_matrix,self.weights)\n",
    "            \n",
    "            if final_likelihood-initial_likelihood <= self.conv_threshold: #convergence criterion\n",
    "                ''' Covergence to be achived when change in log-likelihood is less than convergence threshold'''\n",
    "                self.converged_ = True\n",
    "                break\n",
    "        \n",
    "        if self.converged_ == False:\n",
    "            raise ValueError('Max iteration reached, either increase number of EM iteration or change conv_threshold')\n",
    "        \n",
    "    def predict(self,dataset):\n",
    "        '''Prediction of cluster for each point'''\n",
    "        # scaling of the dataset\n",
    "        \n",
    "        prediction = []\n",
    "        scaled_dataset = self.scaler.transform(dataset)\n",
    "        for point in scaled_dataset:\n",
    "            cluster_prob = [multivariate_normal.pdf(point,mean,covariance) for mean,covariance in zip(self.cluster_centroid,self.covariance_matrix)]\n",
    "            cluster = cluster_prob.index(max(cluster_prob))\n",
    "            prediction.append(cluster)\n",
    "        \n",
    "        return prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
